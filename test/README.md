# BEATs Library Test Suite

This folder contains comprehensive tests for the BEATs feature extraction library.

## ðŸ§ª Test Structure

### Test Files

- **`test_feature_extractor.py`** - Tests for the `BEATsFeatureExtractor` class
- **`test_checkpoint_utils.py`** - Tests for checkpoint management utilities
- **`test_beats_model.py`** - Tests for the core BEATs model functionality
- **`test_cli_and_tools.py`** - Tests for CLI tools and usage patterns
- **`test_integration.py`** - End-to-end integration tests and real-world scenarios
- **`conftest.py`** - Test configuration, fixtures, and utilities
- **`run_tests.py`** - Test runner with different execution modes

### Test Categories

- **Unit Tests**: Fast, isolated tests for individual components
- **Integration Tests**: Tests for complete workflows and component interaction
- **CLI Tests**: Tests for command-line tools and scripts
- **Performance Tests**: Memory usage and timing tests (marked as `slow`)
- **GPU Tests**: CUDA-specific tests (marked as `gpu`, auto-skipped if no GPU)

## ðŸš€ Running Tests

### Quick Start

```bash
# Run all tests
uv run python test/run_tests.py

# Run only fast unit tests
uv run python test/run_tests.py unit

# Run integration tests
uv run python test/run_tests.py integration

# Run quick development tests (stop on first failure)
uv run python test/run_tests.py quick
```

### Using pytest directly

```bash
# Run all tests with verbose output
uv run pytest test/ -v

# Run specific test file
uv run pytest test/test_feature_extractor.py -v

# Run tests matching pattern
uv run pytest test/ -k "checkpoint" -v

# Run with coverage
uv run pytest test/ --cov=beats_trainer --cov-report=html
```

### Test Markers

- `@skip_if_no_model` - Skip if BEATs model not available
- `@skip_if_no_gpu` - Skip if CUDA GPU not available
- `@pytest.mark.slow` - Mark as slow test (excluded in quick runs)

## ðŸ“‹ Test Coverage

### Feature Extractor Tests

- âœ… Initialization with automatic checkpoint detection
- âœ… Different pooling methods (mean, max, cls)
- âœ… Single audio and batch feature extraction
- âœ… File-based feature extraction
- âœ… Feature normalization options
- âœ… Device selection (CPU/GPU/auto)
- âœ… Error handling for invalid inputs
- âœ… Memory management and cleanup

### Checkpoint Management Tests

- âœ… Listing available models
- âœ… Finding existing checkpoints
- âœ… Downloading models from Microsoft Research
- âœ… Checkpoint validation
- âœ… Automatic checkpoint ensuring
- âœ… Error handling for network issues
- âœ… Directory priority handling

### Model Tests

- âœ… Model loading and initialization
- âœ… Forward inference
- âœ… Different input lengths
- âœ… Batch processing
- âœ… GPU inference (if available)
- âœ… Deterministic outputs
- âœ… Gradient flow for training

### Integration Tests

- âœ… Complete feature extraction workflows
- âœ… Multi-model comparisons
- âœ… Batch processing pipelines
- âœ… Save/load feature workflows
- âœ… Error recovery scenarios
- âœ… Real-world application scenarios

### CLI and Tools Tests

- âœ… Command-line interface functionality
- âœ… Library import structure
- âœ… Error handling patterns
- âœ… Configuration validation
- âœ… Documentation consistency
- âœ… Performance characteristics

## ðŸ› ï¸ Test Configuration

### Test Data

Tests use synthetic audio data by default to ensure reproducibility:

- **Synthetic Audio**: Generated sine waves with harmonics and noise
- **Multiple Frequencies**: Different classes have different frequency profiles
- **Batch Audio**: Arrays of synthetic audio for batch testing
- **Mock Files**: Temporary audio files for file-based testing

### Fixtures

- `temp_dir` - Temporary directory for test files
- `synthetic_audio` - Single synthetic audio array
- `batch_synthetic_audio` - Batch of synthetic audio arrays
- `test_audio_files` - Mock audio files for testing

### Configuration

```python
# Test parameters (conftest.py)
TEST_SAMPLE_RATE = 16000
TEST_DURATION = 2.0
TEST_FEATURE_DIM = 768
TEST_BATCH_SIZE = 4
```

## âš¡ Performance Testing

### Memory Tests

- Import memory usage monitoring
- Feature extraction memory tracking
- Memory leak detection
- GPU memory management (if available)

### Timing Tests

- Library import timing
- Feature extraction speed
- Batch processing efficiency
- Model loading performance

### CPU Usage Tests

- CPU utilization monitoring
- Resource usage patterns
- Background process detection

## ðŸŽ¯ Real-World Scenarios

### Audio Similarity Search

Tests complete workflow for building similarity search:
- Feature database creation
- Query processing
- Similarity computation
- Top-K retrieval

### Audio Classification

Tests transfer learning scenario:
- Feature extraction from pretrained model
- Classifier training on features
- Performance evaluation
- Feature importance analysis

### Audio Clustering

Tests unsupervised learning:
- Feature-based clustering
- Cluster quality evaluation
- Cluster analysis and interpretation

## ðŸ”§ Development Workflow

### Running Tests During Development

```bash
# Quick feedback during development
uv run python test/run_tests.py quick

# Test specific component you're working on
uv run pytest test/test_feature_extractor.py::TestBEATsFeatureExtractor::test_extract_features_single_audio -v

# Run with debugger on failure
uv run pytest test/test_feature_extractor.py --pdb
```

### Adding New Tests

1. **Unit Tests**: Add to appropriate `test_*.py` file
2. **Use Fixtures**: Leverage existing fixtures from `conftest.py`
3. **Mock External Dependencies**: Use mocks for file I/O, downloads, etc.
4. **Test Error Cases**: Include both success and failure scenarios
5. **Add Markers**: Use `@skip_if_no_model` for tests requiring models

### Test Writing Guidelines

- **Descriptive Names**: Test names should clearly describe what's being tested
- **Single Responsibility**: Each test should test one specific behavior
- **Mock External Resources**: Don't depend on internet, real files, etc.
- **Cleanup**: Use fixtures and context managers for proper cleanup
- **Assertions**: Include descriptive assertion messages

## ðŸ“Š Coverage Goals

- **Feature Extractor**: > 90% coverage
- **Checkpoint Utils**: > 85% coverage
- **Model Interface**: > 80% coverage
- **CLI Tools**: > 70% coverage
- **Integration**: > 60% coverage

## ðŸš¨ Continuous Integration

Tests are designed to run in CI environments:

- **No External Dependencies**: All tests use mocks or synthetic data
- **Timeout Protection**: Long-running tests have timeouts
- **Platform Independence**: Tests work on Linux, macOS, Windows
- **Resource Awareness**: Graceful degradation when GPU/models unavailable

Run the test suite to ensure your BEATs library is working correctly! ðŸŽµâœ…
